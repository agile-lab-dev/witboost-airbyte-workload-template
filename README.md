# File to Snowflake Airbyte connection template

Use this template to automatically create a connection between a file and a Snowflake instance. Airbyte will take care of creating the Snowflake table, identifying the data schema based on the Pandas IO Tools.

## Using the template

### Prerequisites

A Data Product should already exist in order to attach the new components to it.

---

### Component basic information

This section includes the basic information that any Component of Data Mesh Boost must have

- Name: Required name used for display purposes on your Data Product
- Fully Qualified Name: Workload fully qualified name, this is optional as will be generated by the system if not given by you
- Description: A short description to help others understand what this Workload is for.
- Domain: The Domain of the Data Product this Workload belongs to. Be sure to choose it correctly as is a fundamental part of the Workload and cannot be changed afterwards.
- Data Product: The Data Product this Workload belongs to, be sure to choose the right one.
- Identifier: Unique ID for this new entity inside the domain. Don't worry to fill this field, it will be automatically filled for you.
- Development Group: Development group of this Data Product. Don't worry to fill this field, it will be automatically filled for you.


*Example:*

**Name:** Airbyte Google Covid Workload

**Description:** Loads the public Google Covid Dataset v3 to a Snowflake table

**Domain:** domain:marketing

**Data Product:** system:marketing.covidrecoveries.0

***Identifier:*** Will look something like this: *marketing.covidrecoveries.0.airbyte-google-covid-workload*

***Development Group:*** Might look something like this: *group:dataanalysis* Depends on the Data Product development group 

---

### Source File Details

This section will help Airbyte locate the file and how to read it. Right now we only support files read through HTTPS. For more information on this fields, refer to the [Airbyte Files Source Documentation](https://docs.airbyte.com/integrations/sources/file).

- Source name: A name to identify the Source on the Airbyte environment
- URL: The URL path to access the file to be replicated
- File Format: The format of the file to be replicated. Be aware that some formats may be experimental, please refer to the Airbyte docs. The default value is CSV
- Storage Provider: The storage Provider or Location of the file(s) which should be replicated. Only HTTPS is supported for now.
- User-Agent: Whether to add User-Agent to the HTTPS request
- Dataset Name: The name of the final table to replicate this file into (should include letters, numbers, dash and underscores only).

*Example:*

**Source name:** GoogleCovidDataset

**URL:** https://storage.googleapis.com/covid19-open-data/v3/latest/epidemiology.csv

**File Format:** csv

**Storage Provider:** HTTPS: Public Web

**User-Agent:** ‚ùé

**Dataset Name:** google_covid_dataset

---

### Snowflake Destination details

This section will setup the Snowflake destination and tell Airbyte where to store the data.

- Destination name: A name to identify the Destination on the Airbyte environment
- Database: (Optional) Enter the name of the database you want to sync data into. If not specified the Domain name will be set as default value.
- Schema: (Optional) Enter the name of the default schema. If not specified the value `<DP_name>_<DP_version>` will be set as default value. (The version will be rendered without the dots)

*Example:*

**Destination name:** Snowflake

**Database:** AIRBYTEDB

**Schema:** default

---

### Connection details

Lastly, we need to give a custom name to the connection between the File and Snowflake.

- Connection Name: A name to identify the connection on the Airbyte environment. A common format is "Source Name \<> Destination Name".

**Remember this connection name, as you will need to input it again exactly the same when setting up the MWAA workload.**

*Example:*

**Connection Name:** GoogleCovidDataset \<> Snowflake

### Choose a location

Every component needs a host where the generated code will be saved. The default is `gitlab.com`. Refer to your team to understand the structure on how to use your repository to save this components.

- Host: The host where the repository will be created. By default is `gitlab.com`
- User/Group: A group or a user that the repository belongs to, e.g. 'group/sub-group/sub-sub-group' or 'name.surname'. There are two ways of creating a new component. One way of creating it is by making a monorepo (in that way you will never change the field 'Repository' and it will always be a repository containing your data product and you will only need to change the root directory). The second way of creating a new component is by doing it always in another repository (in that case the root directory will always be '.' and you will always change the repository field).
- Repository: The name of the repository
- Root Directory: The path that will be used as the repository root for this component. For monorepos this will be different for each component.

*Example:*

***Host:*** gitlab.com

**User/Group:** MyCompany/Sandbox

**Repository:** AirbyteSandboxComponent

**RootDirectory:** .

After this the system will show you the summary of the template and you can go back and edit or go ahead and create the Component. After clicking on "Create" the registering of the Component will start. If no errors occurred it will go through the 3 phases (Fetching, Publishing and Registering) and will give you the links to the newly created Repository and the component in the Catalog.