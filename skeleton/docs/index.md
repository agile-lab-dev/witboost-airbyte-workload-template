${{ values.name }}

Use this template to automatically create an Airbyte connection between a source file and a Snowflake instance. Airbyte will take care of creating the Snowflake table, identifying the data schema based on the Pandas IO Tools.

${{ values.description }}

## Using the template

### Prerequisites

* A Data Product should already exist in order to attach the new components to it.
* If you're planning on using a DBT transformation in your ingestion process with Airbyte, be sure to create the DBT component first.
---


### Component basic information

This section includes the basic information that any Component of Data Mesh Boost must have

- Name: Required name used for display purposes on your Data Product
- Fully Qualified Name: Workload fully qualified name, this is optional as will be generated by the system if not given by you
- Description: A short description to help others understand what this Workload is for.
- Domain: The Domain of the Data Product this Workload belongs to. Be sure to choose it correctly as is a fundamental part of the Workload and cannot be changed afterwards.
- Data Product: The Data Product this Workload belongs to, be sure to choose the right one.
- Identifier: Unique ID for this new entity inside the domain. Don't worry to fill this field, it will be automatically filled for you.
- Development Group: Development group of this Data Product. Don't worry to fill this field, it will be automatically filled for you.
- Depends On: If you want your workload to depend on other components from the Data Product, you can choose this option.
  
    > For this component, you should add the **Snowflake Storage** component as a dependency. If you had created a **DBT Component** you should include it here as well.


*Example:*

| Field name              | Example value                                                                                          |
|:------------------------|:-------------------------------------------------------------------------------------------------------|
| **Name**                | Airbyte Vaccinations Ingestion                                                                         |
| **Description**         | Ingestion of vaccinations data                                                                         |
| **Domain**              | domain:healthcare                                                                                      |
| **Data Product**        | system:healthcare.vaccinationsdp.0                                                                     |
| ***Identifier***        | Will look something like this: *healthcare.vaccinationsdp.0.airbyte-vaccinations-ingestion*            |
| ***Development Group*** | Might look something like this: *group:datameshplatform* Depends on the Data Product development group | 
| **Depends On**          | urn:dmb:cmp:healthcare:vaccinationsdp:0:snowflake-vaccinations-storage                                 |

---

### Source File Details

This section will help Airbyte locate the file and how to read it. Right now we only support files read through HTTPS. For more information on these fields, refer to the [Airbyte Files Source Documentation](https://docs.airbyte.com/integrations/sources/file).

- Source name: A name to identify the Source on the Airbyte environment
- URL: The URL path to access the file to be replicated
- File Format: The format of the file to be replicated. Be aware that some formats may be experimental, please refer to the Airbyte docs. The default value is CSV
- Storage Provider: The storage Provider or Location of the file(s) which should be replicated. Only HTTPS is supported for now.
- User-Agent: Whether to add User-Agent (HTTP header that lets servers and network peers identify information about the requesting user) to the HTTPS request
- Dataset Name: The name of the final table to replicate this file into (should include letters, numbers, dash and underscores only).

*Example:*

| Field name           | Example value                                                               |
|:---------------------|:----------------------------------------------------------------------------|
| **Source name**      | Vaccinations                                                                |
| **URL**              | https://storage.googleapis.com/covid19-open-data/v3/latest/vaccinations.csv |
| **File Format**      | csv                                                                         |
| **Storage Provider** | HTTPS: Public Web                                                           |
| **User-Agent**       | ❎                                                                           |
| **Dataset Name**     | Vaccinations_raw                                                            |

---

### Snowflake Destination details

This section will set up the Snowflake destination and tell Airbyte where to store the data. Airbyte by default will create 2 tables in the Snowflake instance: A raw table with the data in JSON format and a normalized table with the values in columns ready to be used.

- Destination name: A name to identify the Destination on the Airbyte environment
- Database: (Optional) Enter the name of the database you want to sync data into. If left empty the Domain name will be set as default value.
- Schema: (Optional) Enter the name of the default schema. If left empty the value `<DP_name>_<DP_majorversion>` will be set as default value.

*Example:*

| Field name           | Example value  |
|:---------------------|:---------------|
| **Destination name** | Snowflake      |
| **Database**         | HEALTHCARE     |
| **Schema**           | vaccinationsdp |

---

### Connection details

Lastly, we need to give a custom name to the connection between the File and Snowflake.

- Connection Name: A name to identify the connection on the Airbyte environment. A common format is "Source Name <> Destination Name".
- Add a dbt transformation (Optional): If you have a dbt project you want to attach to the Airbyte ingestion process (as the T in the ELT process), check this box and fill the following fields:
    - dbt Component: The dbt component already existent in the Data Product
    - dbt git URL: The git url of the dbt repository. This is filled depending on the chosen component.

> Remember the connection name, as you will need to connect to Airbyte on the MWAA script.

*Example __without__ dbt:*

| Field name                   | Example value             |
|:-----------------------------|:--------------------------|
| **Connection Name**          | Vaccinations <> Snowflake |
| **Add a dbt transformation** | ❎                         |

*Example __with__ dbt:*

| Field name                   | Example value                                                                                    |
|:-----------------------------|:-------------------------------------------------------------------------------------------------|
| **Connection Name**          | Vaccinations <> Snowflake                                                                        |
| **Add a dbt transformation** | ✅                                                                                                |
| **dbt Component**            | urn:dmp:cmp:healthcare:vaccinationsdp:0:vaccinations-dbt-workload                                |
| ***dbt git URL***            | *https://gitlab.com/MyCompany/mesh.repository/sandbox/vaccinations/VaccinationsDBTComponent.git* |

---

### Choose a location

Every component needs a host where the generated code will be saved. The default is `gitlab.com`. Refer to your team to understand the structure on how to use your repository to save these components.

- Host: The host where the repository will be created. By default is `gitlab.com`
- User/Group: A group or a user that the repository belongs to, e.g. 'group/sub-group/sub-sub-group' or 'name.surname'. There are two ways of creating a new component. One way of creating it is by making a monorepo (in that way you will never change the field 'Repository' and it will always be a repository containing your data product, and you will only need to change the root directory). The second way of creating a new component is by doing it always in another repository (in that case the root directory will always be '.' and you will always change the repository field).
- Repository: The name of the repository
- Root Directory: The path that will be used as the repository root for this component. For monorepos this will be different for each component.

*Example:*

| Field name        | Example value                                  |
|:------------------|:-----------------------------------------------|
| ***Host***        | gitlab.com                                     |
| **User/Group**    | MyCompany/mesh.repository/sandbox/vaccinations |
| **Repository**    | AirbyteVaccinationsIngestion                   |
| **RootDirectory** | .                                              |

After this the system will show you the summary of the template, and you can go back and edit or go ahead and create the Component.

After clicking on "Create" the registering of the Component will start. If no errors occurred it will go through the 3 phases (Fetching, Publishing and Registering) and will give you the links to the newly created Repository and the component in the Catalog.

After this, we can move on to either creating the Snowflake SQL Workload if you haven't defined a data transformation, or directly to creating the Snowflake Output Port if you have.
